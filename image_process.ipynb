{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FCaLykxapHc",
        "outputId": "ece1bb04-633b-4811-855e-aa29da3a7ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Face cropping completed. Cropped images saved in the destination folder.\n"
          ]
        }
      ],
      "source": [
        "# Face Cropping\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "lst = [\"shivam\"]\n",
        "for i in lst:\n",
        "\n",
        "  source_folder = f'/content/drive/Shareddrives/CNN_Project/Photos2026/{i}'\n",
        "  destination_folder = f'/content/drive/Shareddrives/CNN_Project/Photos2026/{i}_Face'\n",
        "\n",
        "  os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "  def crop_faces(input_folder, output_folder, face_size=(128, 128)):\n",
        "      face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "      for img_name in os.listdir(input_folder):\n",
        "          img_path = os.path.join(input_folder, img_name)\n",
        "\n",
        "          img = cv2.imread(img_path)\n",
        "          if img is None:\n",
        "              continue\n",
        "\n",
        "          gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "          faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "          for (x, y, w, h) in faces:\n",
        "              face_img = img[y:y + h, x:x + w]\n",
        "              resized_face = cv2.resize(face_img, face_size)\n",
        "              output_path = os.path.join(output_folder, img_name)\n",
        "              cv2.imwrite(output_path, resized_face)\n",
        "              break\n",
        "\n",
        "  crop_faces(source_folder, destination_folder)\n",
        "\n",
        "  print(\"Face cropping completed. Cropped images saved in the destination folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fyi1At92YQV",
        "outputId": "815d31fb-1428-4c1b-8afe-50d0ad4831d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20240407_134438794.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20240328_105548.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20240705_201413287.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/1676639882315.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20231223_112904.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20231225_112744894.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20240321_184308.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20240303_153544.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20240525_125222.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20240518_191537.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230624_080959.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20240708_194845676.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/1720390672972.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG20230201122723.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230313_124019.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20231225_095949.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230623_184653.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230715_144948.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20240602_201913587.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20240328_180900.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230623_184704.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230805_173704.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20231117_185322.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20241025_220015320.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230217_123820.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20230708_222803813.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230702_164930.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230303_011857.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230815_215219.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230313_123934.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230702_165523.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG20221024194800.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230217_123800.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20221024_091759.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/IMG_20230610_072831.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20230709_130737228.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/1723174231715.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20221119_073136643.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20240417_232520627.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20221014_061425250.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Animesh_Face/Polish_20230901_214552187.jpg\n",
            "Face cropping completed for Animesh. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-2074534742.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-269042869.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-554211984.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1282259713.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1941898614.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1508861443.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-94291527.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-41694306.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-34145379.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1841470847.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1491585745.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-563155242.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/LS20240820232249.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-642255207.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1042141529.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-843518090.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1156221306.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-674135743.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1720934041.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-708118857.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-90942966.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20221007213407.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20221025235722.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20221020142232.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20221007212107.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20241026043225.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1569746800.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-266484378.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-2057629967.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20230101190055.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/Snapchat-1595180953.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20240801223014.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20240819175229.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20240730121455.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20241027125016.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20240806203312.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20241026054403.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Ankita_Face/IMG20240815135427.jpg\n",
            "Face cropping completed for Ankita. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG-20221208-WA0005.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG20221123112630.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230325_164312_282.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221120_195203_627.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG-20221208-WA0012.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221207_235910_508.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/Snapchat-125392329.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/Snapchat-114869666.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230208_185639_201.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/Snapchat-1127883053.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG-20221208-WA0003.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG-20221118-WA0015.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG-20221208-WA0013.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230325_132327_503.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230217_125043_998.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230114_002647_919.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221207_231613_226.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221209_222954_242.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG-20221103-WA0037.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230212_160707_620.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230222_200121_271.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221124_021006_025.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230217_124138_766.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230313_124100.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230309_204146_311.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230325_164313_999.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230207_101957_080.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221023_150038_267.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230313_124203.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230131_134442_640.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230313_123510.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230119_151109_157.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230325_132352_706.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20221208_134603.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG20230318200611.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230324_131931_060.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230214_095747_458.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20230204_124700_298.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Anmol_Face/IMG_20240411_193842.jpg\n",
            "Face cropping completed for Anmol. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230622_111249.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240124001737.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230702_131443.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/Screenshot_2023-08-06-20-57-44-000_com.google.android.apps.photos-edit.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/Screenshot_2024-01-20-20-38-31-53_6012fa4d4ddec268fc5c7112cbb265e7.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG-20230831-WA0002.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/Screenshot_2023-08-06-18-37-30-455_com.google.android.apps.photos-edit.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG-20240308-WA0006.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/Screenshot_2023-09-12-10-15-38-184_com.google.android.apps.photos-edit.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20231108_211147.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/Screenshot_2022-12-13-22-56-29-286_com.whatsapp.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/Screenshot_2023-05-04-18-06-16-688_com.whatsapp.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20231030_002537.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG-20231002-WA0000.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20241024130037_BURST000_COVER.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240107002045.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240929211435.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230830_203317.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20241024141105.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20231209162821.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20241030165334.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230531_021125.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20240422_184057(1).png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230612_092218~2.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230730_174319.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20231116122116.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240127205229.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG_20230803_155015.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20241024153450.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240804043442.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240804043436.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240717155657.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240727222121.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20231230101912.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20241024134217.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20241024172507.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240617210744.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240216210914.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240630145103.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240819121608.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240819142900.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Deependra_Face/IMG20240819134513.jpg\n",
            "Face cropping completed for Deependra. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231117_092156.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20240606_174736.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230624_113606.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230827_093434.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG-20240810-WA0002.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231228_123239.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230119_131109.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231117_092610.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230914_201912.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231117_085238.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231201_215159.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG-20240812-WA0016.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20240804_133903.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231117_093634.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231201_215147.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230802_102714.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231222_091325.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230914_201931.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20240414_181804.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20240804_133835.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230904_192007.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230816_110053.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231104_134139.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230610_094808.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20240804_140536.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20240606_174839.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230624_112035.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230904_191946.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20231117_093731.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230928_172121.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230928_174533.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230928_183321.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230624_115425.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230612_112931.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230729_182610.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230121_172706.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230624_121241.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230928_174544.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Narayan_Face/IMG_20230309_092459.jpg\n",
            "Face cropping completed for Narayan. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240414185911.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240310182840.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240921191150.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240414185027.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240412112447.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240705105701.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240821210310.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240705110120.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240705110353.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231015192530.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240722095554.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240412112120.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231110151257.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240705105942.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240506072740.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240921191513.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240705110040.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240722095603.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240414183202.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20241102183937.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240821205827.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231011155503.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231107122617.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240414184938.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240412112243.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240722095556.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240414183933.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231010131804.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20241007183859.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240921192002.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240418064646.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240418064725.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240412112139.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231226150514.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231107122608.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240412112153.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231107122623.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240418064723.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231107122627.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20240821205814.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231226150515.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231107122628.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Pankaj_Face/IMG20231107122616.jpg\n",
            "Face cropping completed for Pankaj. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20230118_123844.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231018073729.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20240721_160930.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20240528_103533.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20230605_185606.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20230603_211026.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20240429_184502.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20240604_120343.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG_20240918_080600.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231023123857.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231112171243.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231112171301.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231114150319.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231112171722.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231112172014.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240322102604.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240528080038.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240529122220.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240529122310.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240603083141.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20231112171341.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240603092315.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240623164558.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240625131049.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20241002091141.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240813091439.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240916230909.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240721112816.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240826161605.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20241002091042.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240813225919.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240813222923.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of IMG20240721115116.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of SAVE_20230821_085719.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of Screenshot_2022-11-15-18-16-22-661_com.whatsapp.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of Snapchat-355948997.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of Snapchat-994168737.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of Snapchat-1060098721.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sahil_Face/Copy of Snapchat-1296370789.jpg\n",
            "Face cropping completed for Sahil. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_181813.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_182213.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_181812.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_184440.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_181810.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_192145.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_192146.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_211638.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_193642.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_193629.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_211633.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221027_143605.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_184443.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221027_143609.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_192157.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221027_143610.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_211423.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221026_172412.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221027_143608.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_211424.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_210451.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_201113.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221026_172414.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_210444.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221027_143606.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221103_182211.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_210501.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221021_154422.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_211311.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_210456.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_134952.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221016_133653.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_142710.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221021_154419.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_142713.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221016_131914.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_135025.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_210317.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221024_210454.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221016_133739.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_135009.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_085245.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_085233.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_085251.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_134956.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_135027.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221016_133736.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221016_131909.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_142712.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221016_131911.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_142714.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_111635.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221018_135006.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_085242.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221007_151741.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221007_151737.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_085240.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221007_151746.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20220905_212504.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20220905_212454.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20220905_212452.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20220905_212458.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221007_151748.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20221010_111640.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/IMG_20220908_190003.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/Snapchat-1558549179.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/Snapchat-866754511.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Sonal_Face/Snapchat-1613608992.jpg\n",
            "Face cropping completed for Sonal. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000056508.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000057521.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of IMG-20241104-WA0007.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000045019.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000056159.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of IMG_20241026_162933.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000042424.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000042383.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000042401.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000042414.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000042388.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000039976.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000042352.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000039988.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000037727.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000032548.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000037693.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000018196.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000016223.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000007193.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000006243.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000006251.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000021366.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000023314.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000023591.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000003472.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000003661.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000003593.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000003834.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000005899.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Suraj_Face/Copy of 1000003852.jpg\n",
            "Face cropping completed for Suraj. Cropped images saved in the destination folder.\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20210308_200205.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241026061802.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Screenshot_2022-08-31-13-06-00-91.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG-20221115-WA0002.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG-20221117-WA0011.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/BeautyPlus_20201103124644372_save.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20210227_202426.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20241110_111057.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Snapchat-1881214228.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Snapchat-1868308529.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG-20221122-WA00311.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Snapchat-1054145203.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20210208_184704.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20220907_074119.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Screenshot_2022-11-22-09-11-59-82.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG-20241105-WA0019.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Screenshot_2021-01-14-14-21-37-14.png\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20210111_080519.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20201210_114023.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Snapchat-1085839008.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20220901_195158.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/Snapchat-1986405584.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG_20230831_210911.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241017190708.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241107153751.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241107154424.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241012112546.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241012112541.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20240101103942.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241017191646.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241019194003.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241016192344.jpg\n",
            "Cropped face saved: /content/drive/Shareddrives/CNN_Project/Photos2026/Vidya_Face/IMG20241107154421.jpg\n",
            "Face cropping completed for Vidya. Cropped images saved in the destination folder.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "lst = [\"Animesh\", \"Ankita\", \"Anmol\", \"Deependra\", \"Narayan\", \"Pankaj\", \"Sahil\", \"Sonal\", \"Suraj\", \"Vidya\"]\n",
        "\n",
        "for i in lst:\n",
        "    source_folder = f'/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/{i}'\n",
        "    destination_folder = f'/content/drive/Shareddrives/CNN_Project/Photos2026/{i}_Face'\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "    def crop_faces_dnn(input_folder, output_folder, face_size=(128, 128), padding=10):\n",
        "        # Provide the correct paths to the model and config files\n",
        "        modelFile = '/content/drive/Shareddrives/CNN_Project/FaceModel/res10_300x300_ssd_iter_140000.caffemodel'\n",
        "        configFile = '/content/drive/Shareddrives/CNN_Project/FaceModel/deploy.prototxt'\n",
        "        net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
        "\n",
        "        for img_name in os.listdir(input_folder):\n",
        "            img_path = os.path.join(input_folder, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Could not read image {img_name}\")\n",
        "                continue\n",
        "\n",
        "            h, w = img.shape[:2]\n",
        "            blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), [104, 117, 123], False, False)\n",
        "\n",
        "            # Detect faces\n",
        "            net.setInput(blob)\n",
        "            detections = net.forward()\n",
        "\n",
        "            for i in range(detections.shape[2]):\n",
        "                confidence = detections[0, 0, i, 2]\n",
        "                if confidence > 0.5:  # Adjust threshold for better results\n",
        "                    box = detections[0, 0, i, 3:7] * [w, h, w, h]\n",
        "                    (x1, y1, x2, y2) = box.astype(\"int\")\n",
        "\n",
        "                    # Add padding\n",
        "                    x1 = max(0, x1 - padding)\n",
        "                    y1 = max(0, y1 - padding)\n",
        "                    x2 = min(w, x2 + padding)\n",
        "                    y2 = min(h, y2 + padding)\n",
        "\n",
        "                    # Crop face and resize\n",
        "                    face_img = img[y1:y2, x1:x2]\n",
        "                    resized_face = cv2.resize(face_img, face_size)\n",
        "\n",
        "                    # Save cropped face\n",
        "                    output_path = os.path.join(output_folder, img_name)\n",
        "                    cv2.imwrite(output_path, resized_face)\n",
        "                    print(f\"Cropped face saved: {output_path}\")\n",
        "                    break  # Process only the first face detected\n",
        "\n",
        "    crop_faces_dnn(source_folder, destination_folder)\n",
        "    print(f\"Face cropping completed for {i}. Cropped images saved in the destination folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GxH89aI7JHT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjzyK9GFzPGu",
        "outputId": "2f0d1fbf-9ad0-4511-c1ad-102dc7648537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install facenet-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfQYBUeUpAdw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "# Base class to define the basic setup of the model\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError(\"Forward method not implemented!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ6aoP7q2Vxf",
        "outputId": "7c157af7-aa27-4e10-939c-5d8950b40e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJVK2_lnzCtL"
      },
      "outputs": [],
      "source": [
        "class FaceRecognitionModel(BaseModel):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FaceRecognitionModel, self).__init__()\n",
        "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "        for param in self.facenet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Placeholder for fc1, updated dynamically\n",
        "        self.fc1 = None\n",
        "        self.fc2 = nn.Linear(256, num_classes)  # fc2 remains the same\n",
        "\n",
        "    def initialize_fc1(self, input_shape):\n",
        "        # Create a dummy input with the correct shape and device\n",
        "        dummy_input = torch.randn(input_shape).to(next(self.parameters()).device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_output = self.facenet(dummy_input)\n",
        "\n",
        "        # Calculate the flattened size after passing through the convolutional layers\n",
        "        conv1_output = self.conv1(dummy_output.unsqueeze(-1).unsqueeze(-1))\n",
        "        conv2_output = self.conv2(conv1_output)\n",
        "        flat_size = conv2_output.view(conv2_output.size(0), -1).size(1)\n",
        "\n",
        "        # Initialize fc1 with the calculated input size\n",
        "        self.fc1 = nn.Linear(flat_size, 256).to(next(self.parameters()).device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.fc1 is None:\n",
        "            raise ValueError(\"fc1 is not initialized. Call `initialize_fc1` with the input shape.\")\n",
        "\n",
        "        # Move input to the same device as the model parameters\n",
        "        x = x.to(next(self.parameters()).device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.facenet(x)\n",
        "\n",
        "        x = self.conv1(x.unsqueeze(-1).unsqueeze(-1))\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def fine_tune(self, train_loader, val_loader, num_epochs=10, lr=0.001):\n",
        "        # Define loss and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for images, labels in train_loader:\n",
        "                # Move data to the same device as the model\n",
        "                device = next(self.parameters()).device\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Validation loop\n",
        "            self.eval()\n",
        "            val_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = self(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item() * images.size(0)\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            # Print statistics\n",
        "            train_loss /= len(train_loader.dataset)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            val_accuracy = 100 * correct / total\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
        "                  f\"Train Loss: {train_loss:.4f}, \"\n",
        "                  f\"Validation Loss: {val_loss:.4f}, \"\n",
        "                  f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qDsCrZXSCSq"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        for idx, class_name in enumerate(os.listdir(root_dir)):\n",
        "            self.class_to_idx[class_name] = idx\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                self.images.append(os.path.join(class_dir, img_name))\n",
        "                self.labels.append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load training and validation data\n",
        "train_dataset = FaceDataset('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train', transform=transform)\n",
        "val_dataset = FaceDataset('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/val', transform=transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzRw5k41463b"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laGFVcHeTWdk",
        "outputId": "259502a9-22fb-4b80-d1d8-6a803d173494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8V1ptihVF-q",
        "outputId": "d7298123-d778-4231-ca67-70ce42862766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Train Loss: 2.3845, Validation Loss: 2.3532, Validation Accuracy: 18.18%\n",
            "Epoch 2/10, Train Loss: 2.2367, Validation Loss: 2.0711, Validation Accuracy: 27.27%\n",
            "Epoch 3/10, Train Loss: 1.6969, Validation Loss: 1.3986, Validation Accuracy: 67.27%\n",
            "Epoch 4/10, Train Loss: 0.9851, Validation Loss: 0.6241, Validation Accuracy: 85.45%\n",
            "Epoch 5/10, Train Loss: 0.4453, Validation Loss: 0.2616, Validation Accuracy: 96.36%\n",
            "Epoch 6/10, Train Loss: 0.2370, Validation Loss: 0.1243, Validation Accuracy: 96.36%\n",
            "Epoch 7/10, Train Loss: 0.1464, Validation Loss: 0.0893, Validation Accuracy: 96.36%\n",
            "Epoch 8/10, Train Loss: 0.0963, Validation Loss: 0.1048, Validation Accuracy: 94.55%\n",
            "Epoch 9/10, Train Loss: 0.0901, Validation Loss: 0.0776, Validation Accuracy: 96.36%\n",
            "Epoch 10/10, Train Loss: 0.0769, Validation Loss: 0.0793, Validation Accuracy: 96.36%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the model\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=10, lr=0.001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb34ctaX5Fu4",
        "outputId": "b8ed3f56-fe16-4e51-aa39-043784862ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/10, Train Loss: 2.3524, Validation Loss: 2.2436, Validation Accuracy: 14.55%\n",
            "Epoch 2/10, Train Loss: 1.6969, Validation Loss: 1.1433, Validation Accuracy: 65.45%\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=10, lr=0.001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd0VghO45Z6H",
        "outputId": "8de6fc5b-2358-487e-99d8-a8adb0b56a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/10, Train Loss: 2.1555, Validation Loss: 1.7818, Validation Accuracy: 41.82%\n",
            "Epoch 2/10, Train Loss: 1.2971, Validation Loss: 0.9896, Validation Accuracy: 67.27%\n",
            "Epoch 3/10, Train Loss: 0.9296, Validation Loss: 0.5916, Validation Accuracy: 87.27%\n",
            "Epoch 4/10, Train Loss: 0.7186, Validation Loss: 0.2760, Validation Accuracy: 96.36%\n",
            "Epoch 5/10, Train Loss: 0.5095, Validation Loss: 0.2283, Validation Accuracy: 94.55%\n",
            "Epoch 6/10, Train Loss: 0.3705, Validation Loss: 0.5605, Validation Accuracy: 83.64%\n",
            "Epoch 7/10, Train Loss: 0.3945, Validation Loss: 0.2881, Validation Accuracy: 89.09%\n",
            "Epoch 8/10, Train Loss: 0.3664, Validation Loss: 0.1907, Validation Accuracy: 94.55%\n",
            "Epoch 9/10, Train Loss: 0.3923, Validation Loss: 0.1794, Validation Accuracy: 92.73%\n",
            "Epoch 10/10, Train Loss: 0.2506, Validation Loss: 0.2514, Validation Accuracy: 90.91%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=10, lr=0.001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrmWOSe-5hnV",
        "outputId": "2564a7c2-c862-45b1-8cc9-1a1526822094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/10, Train Loss: 2.3890, Validation Loss: 2.3931, Validation Accuracy: 9.09%\n",
            "Epoch 2/10, Train Loss: 2.3665, Validation Loss: 2.3544, Validation Accuracy: 23.64%\n",
            "Epoch 3/10, Train Loss: 2.2326, Validation Loss: 2.1737, Validation Accuracy: 27.27%\n",
            "Epoch 4/10, Train Loss: 1.9291, Validation Loss: 1.8951, Validation Accuracy: 29.09%\n",
            "Epoch 5/10, Train Loss: 1.6290, Validation Loss: 1.4489, Validation Accuracy: 52.73%\n",
            "Epoch 6/10, Train Loss: 1.3880, Validation Loss: 1.1952, Validation Accuracy: 56.36%\n",
            "Epoch 7/10, Train Loss: 1.1941, Validation Loss: 0.9176, Validation Accuracy: 74.55%\n",
            "Epoch 8/10, Train Loss: 0.9910, Validation Loss: 0.7975, Validation Accuracy: 80.00%\n",
            "Epoch 9/10, Train Loss: 0.9156, Validation Loss: 0.7030, Validation Accuracy: 80.00%\n",
            "Epoch 10/10, Train Loss: 0.8609, Validation Loss: 0.7267, Validation Accuracy: 81.82%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=10, lr=0.0001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCGjPTuN5msk",
        "outputId": "58bc0c21-7c83-4e09-8f86-19445a0daf16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/50, Train Loss: 2.3973, Validation Loss: 2.3919, Validation Accuracy: 9.09%\n",
            "Epoch 2/50, Train Loss: 2.3732, Validation Loss: 2.3461, Validation Accuracy: 32.73%\n",
            "Epoch 3/50, Train Loss: 2.2257, Validation Loss: 2.1465, Validation Accuracy: 36.36%\n",
            "Epoch 4/50, Train Loss: 1.8926, Validation Loss: 1.7667, Validation Accuracy: 41.82%\n",
            "Epoch 5/50, Train Loss: 1.5750, Validation Loss: 1.4436, Validation Accuracy: 50.91%\n",
            "Epoch 6/50, Train Loss: 1.3164, Validation Loss: 1.3430, Validation Accuracy: 47.27%\n",
            "Epoch 7/50, Train Loss: 1.2026, Validation Loss: 1.0567, Validation Accuracy: 60.00%\n",
            "Epoch 8/50, Train Loss: 1.0817, Validation Loss: 0.9182, Validation Accuracy: 69.09%\n",
            "Epoch 9/50, Train Loss: 1.0272, Validation Loss: 0.8743, Validation Accuracy: 67.27%\n",
            "Epoch 10/50, Train Loss: 0.9688, Validation Loss: 0.8679, Validation Accuracy: 65.45%\n",
            "Epoch 11/50, Train Loss: 0.8389, Validation Loss: 0.7738, Validation Accuracy: 72.73%\n",
            "Epoch 12/50, Train Loss: 0.8655, Validation Loss: 0.7584, Validation Accuracy: 74.55%\n",
            "Epoch 13/50, Train Loss: 0.8311, Validation Loss: 0.6959, Validation Accuracy: 81.82%\n",
            "Epoch 14/50, Train Loss: 0.8025, Validation Loss: 0.6167, Validation Accuracy: 87.27%\n",
            "Epoch 15/50, Train Loss: 0.7222, Validation Loss: 0.4920, Validation Accuracy: 94.55%\n",
            "Epoch 16/50, Train Loss: 0.6467, Validation Loss: 0.4428, Validation Accuracy: 89.09%\n",
            "Epoch 17/50, Train Loss: 0.6555, Validation Loss: 0.4862, Validation Accuracy: 90.91%\n",
            "Epoch 18/50, Train Loss: 0.5973, Validation Loss: 0.3734, Validation Accuracy: 94.55%\n",
            "Epoch 19/50, Train Loss: 0.5471, Validation Loss: 0.3429, Validation Accuracy: 92.73%\n",
            "Epoch 20/50, Train Loss: 0.4822, Validation Loss: 0.5217, Validation Accuracy: 83.64%\n",
            "Epoch 21/50, Train Loss: 0.5328, Validation Loss: 0.3077, Validation Accuracy: 96.36%\n",
            "Epoch 22/50, Train Loss: 0.5217, Validation Loss: 0.4178, Validation Accuracy: 89.09%\n",
            "Epoch 23/50, Train Loss: 0.4537, Validation Loss: 0.2941, Validation Accuracy: 94.55%\n",
            "Epoch 24/50, Train Loss: 0.4482, Validation Loss: 0.2592, Validation Accuracy: 94.55%\n",
            "Epoch 25/50, Train Loss: 0.3957, Validation Loss: 0.3553, Validation Accuracy: 89.09%\n",
            "Epoch 26/50, Train Loss: 0.4046, Validation Loss: 0.2414, Validation Accuracy: 96.36%\n",
            "Epoch 27/50, Train Loss: 0.3494, Validation Loss: 0.2952, Validation Accuracy: 96.36%\n",
            "Epoch 28/50, Train Loss: 0.4255, Validation Loss: 0.2800, Validation Accuracy: 94.55%\n",
            "Epoch 29/50, Train Loss: 0.4467, Validation Loss: 0.1989, Validation Accuracy: 96.36%\n",
            "Epoch 30/50, Train Loss: 0.4600, Validation Loss: 0.2230, Validation Accuracy: 96.36%\n",
            "Epoch 31/50, Train Loss: 0.4024, Validation Loss: 0.2683, Validation Accuracy: 94.55%\n",
            "Epoch 32/50, Train Loss: 0.3649, Validation Loss: 0.3298, Validation Accuracy: 85.45%\n",
            "Epoch 33/50, Train Loss: 0.3305, Validation Loss: 0.2040, Validation Accuracy: 96.36%\n",
            "Epoch 34/50, Train Loss: 0.3150, Validation Loss: 0.3778, Validation Accuracy: 83.64%\n",
            "Epoch 35/50, Train Loss: 0.3699, Validation Loss: 0.2535, Validation Accuracy: 94.55%\n",
            "Epoch 36/50, Train Loss: 0.3950, Validation Loss: 0.2293, Validation Accuracy: 94.55%\n",
            "Epoch 37/50, Train Loss: 0.3172, Validation Loss: 0.1274, Validation Accuracy: 98.18%\n",
            "Epoch 38/50, Train Loss: 0.3898, Validation Loss: 0.2216, Validation Accuracy: 96.36%\n",
            "Epoch 39/50, Train Loss: 0.2930, Validation Loss: 0.1565, Validation Accuracy: 98.18%\n",
            "Epoch 40/50, Train Loss: 0.3412, Validation Loss: 0.2620, Validation Accuracy: 92.73%\n",
            "Epoch 41/50, Train Loss: 0.2910, Validation Loss: 0.1678, Validation Accuracy: 96.36%\n",
            "Epoch 42/50, Train Loss: 0.2710, Validation Loss: 0.2267, Validation Accuracy: 94.55%\n",
            "Epoch 43/50, Train Loss: 0.2648, Validation Loss: 0.3100, Validation Accuracy: 89.09%\n",
            "Epoch 44/50, Train Loss: 0.3016, Validation Loss: 0.2863, Validation Accuracy: 89.09%\n",
            "Epoch 45/50, Train Loss: 0.3421, Validation Loss: 0.1900, Validation Accuracy: 96.36%\n",
            "Epoch 46/50, Train Loss: 0.2691, Validation Loss: 0.1368, Validation Accuracy: 98.18%\n",
            "Epoch 47/50, Train Loss: 0.2862, Validation Loss: 0.2898, Validation Accuracy: 89.09%\n",
            "Epoch 48/50, Train Loss: 0.2821, Validation Loss: 0.1492, Validation Accuracy: 98.18%\n",
            "Epoch 49/50, Train Loss: 0.2992, Validation Loss: 0.3229, Validation Accuracy: 90.91%\n",
            "Epoch 50/50, Train Loss: 0.2882, Validation Loss: 0.2148, Validation Accuracy: 92.73%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=50, lr=0.0001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi0bTjA37vXB",
        "outputId": "88c72532-28df-43be-fc26-412611ea9a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/10, Train Loss: 2.3889, Validation Loss: 2.3642, Validation Accuracy: 18.18%\n",
            "Epoch 2/10, Train Loss: 2.2462, Validation Loss: 2.0985, Validation Accuracy: 18.18%\n",
            "Epoch 3/10, Train Loss: 1.7534, Validation Loss: 1.5577, Validation Accuracy: 69.09%\n",
            "Epoch 4/10, Train Loss: 1.1531, Validation Loss: 0.7308, Validation Accuracy: 87.27%\n",
            "Epoch 5/10, Train Loss: 0.5179, Validation Loss: 0.2836, Validation Accuracy: 94.55%\n",
            "Epoch 6/10, Train Loss: 0.2893, Validation Loss: 0.1357, Validation Accuracy: 98.18%\n",
            "Epoch 7/10, Train Loss: 0.1607, Validation Loss: 0.1070, Validation Accuracy: 94.55%\n",
            "Epoch 8/10, Train Loss: 0.1144, Validation Loss: 0.0685, Validation Accuracy: 98.18%\n",
            "Epoch 9/10, Train Loss: 0.1119, Validation Loss: 0.1107, Validation Accuracy: 96.36%\n",
            "Epoch 10/10, Train Loss: 0.0831, Validation Loss: 0.0846, Validation Accuracy: 96.36%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=10, lr=0.001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDXHcK9676rm",
        "outputId": "7a98b263-9396-4460-f93f-d6c237da559d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/50, Train Loss: 2.3992, Validation Loss: 2.3973, Validation Accuracy: 9.09%\n",
            "Epoch 2/50, Train Loss: 2.3961, Validation Loss: 2.3953, Validation Accuracy: 9.09%\n",
            "Epoch 3/50, Train Loss: 2.3922, Validation Loss: 2.3922, Validation Accuracy: 9.09%\n",
            "Epoch 4/50, Train Loss: 2.3868, Validation Loss: 2.3873, Validation Accuracy: 16.36%\n",
            "Epoch 5/50, Train Loss: 2.3783, Validation Loss: 2.3798, Validation Accuracy: 18.18%\n",
            "Epoch 6/50, Train Loss: 2.3666, Validation Loss: 2.3685, Validation Accuracy: 23.64%\n",
            "Epoch 7/50, Train Loss: 2.3488, Validation Loss: 2.3528, Validation Accuracy: 40.00%\n",
            "Epoch 8/50, Train Loss: 2.3243, Validation Loss: 2.3296, Validation Accuracy: 45.45%\n",
            "Epoch 9/50, Train Loss: 2.2869, Validation Loss: 2.2964, Validation Accuracy: 45.45%\n",
            "Epoch 10/50, Train Loss: 2.2296, Validation Loss: 2.2494, Validation Accuracy: 41.82%\n",
            "Epoch 11/50, Train Loss: 2.1527, Validation Loss: 2.1879, Validation Accuracy: 43.64%\n",
            "Epoch 12/50, Train Loss: 2.0515, Validation Loss: 2.1101, Validation Accuracy: 41.82%\n",
            "Epoch 13/50, Train Loss: 1.9576, Validation Loss: 2.0267, Validation Accuracy: 41.82%\n",
            "Epoch 14/50, Train Loss: 1.8404, Validation Loss: 1.9334, Validation Accuracy: 38.18%\n",
            "Epoch 15/50, Train Loss: 1.7574, Validation Loss: 1.8279, Validation Accuracy: 41.82%\n",
            "Epoch 16/50, Train Loss: 1.6430, Validation Loss: 1.7220, Validation Accuracy: 43.64%\n",
            "Epoch 17/50, Train Loss: 1.5337, Validation Loss: 1.6132, Validation Accuracy: 49.09%\n",
            "Epoch 18/50, Train Loss: 1.4305, Validation Loss: 1.4882, Validation Accuracy: 50.91%\n",
            "Epoch 19/50, Train Loss: 1.3124, Validation Loss: 1.3778, Validation Accuracy: 50.91%\n",
            "Epoch 20/50, Train Loss: 1.2056, Validation Loss: 1.2588, Validation Accuracy: 54.55%\n",
            "Epoch 21/50, Train Loss: 1.1063, Validation Loss: 1.1557, Validation Accuracy: 58.18%\n",
            "Epoch 22/50, Train Loss: 0.9974, Validation Loss: 1.0590, Validation Accuracy: 67.27%\n",
            "Epoch 23/50, Train Loss: 0.9061, Validation Loss: 0.9582, Validation Accuracy: 72.73%\n",
            "Epoch 24/50, Train Loss: 0.8241, Validation Loss: 0.8588, Validation Accuracy: 74.55%\n",
            "Epoch 25/50, Train Loss: 0.7486, Validation Loss: 0.7774, Validation Accuracy: 85.45%\n",
            "Epoch 26/50, Train Loss: 0.6808, Validation Loss: 0.6962, Validation Accuracy: 85.45%\n",
            "Epoch 27/50, Train Loss: 0.6367, Validation Loss: 0.6241, Validation Accuracy: 85.45%\n",
            "Epoch 28/50, Train Loss: 0.5484, Validation Loss: 0.5689, Validation Accuracy: 89.09%\n",
            "Epoch 29/50, Train Loss: 0.5111, Validation Loss: 0.5132, Validation Accuracy: 89.09%\n",
            "Epoch 30/50, Train Loss: 0.4617, Validation Loss: 0.4648, Validation Accuracy: 90.91%\n",
            "Epoch 31/50, Train Loss: 0.4209, Validation Loss: 0.4311, Validation Accuracy: 90.91%\n",
            "Epoch 32/50, Train Loss: 0.3798, Validation Loss: 0.3847, Validation Accuracy: 90.91%\n",
            "Epoch 33/50, Train Loss: 0.3442, Validation Loss: 0.3582, Validation Accuracy: 90.91%\n",
            "Epoch 34/50, Train Loss: 0.3300, Validation Loss: 0.3128, Validation Accuracy: 90.91%\n",
            "Epoch 35/50, Train Loss: 0.2902, Validation Loss: 0.2858, Validation Accuracy: 94.55%\n",
            "Epoch 36/50, Train Loss: 0.2871, Validation Loss: 0.2731, Validation Accuracy: 94.55%\n",
            "Epoch 37/50, Train Loss: 0.2643, Validation Loss: 0.2565, Validation Accuracy: 94.55%\n",
            "Epoch 38/50, Train Loss: 0.2514, Validation Loss: 0.2283, Validation Accuracy: 94.55%\n",
            "Epoch 39/50, Train Loss: 0.2357, Validation Loss: 0.2210, Validation Accuracy: 94.55%\n",
            "Epoch 40/50, Train Loss: 0.2280, Validation Loss: 0.2051, Validation Accuracy: 94.55%\n",
            "Epoch 41/50, Train Loss: 0.2055, Validation Loss: 0.1907, Validation Accuracy: 94.55%\n",
            "Epoch 42/50, Train Loss: 0.1937, Validation Loss: 0.1776, Validation Accuracy: 94.55%\n",
            "Epoch 43/50, Train Loss: 0.1817, Validation Loss: 0.2185, Validation Accuracy: 94.55%\n",
            "Epoch 44/50, Train Loss: 0.1761, Validation Loss: 0.1598, Validation Accuracy: 94.55%\n",
            "Epoch 45/50, Train Loss: 0.1680, Validation Loss: 0.1655, Validation Accuracy: 94.55%\n",
            "Epoch 46/50, Train Loss: 0.1558, Validation Loss: 0.1553, Validation Accuracy: 94.55%\n",
            "Epoch 47/50, Train Loss: 0.1611, Validation Loss: 0.1377, Validation Accuracy: 94.55%\n",
            "Epoch 48/50, Train Loss: 0.1717, Validation Loss: 0.1359, Validation Accuracy: 94.55%\n",
            "Epoch 49/50, Train Loss: 0.1371, Validation Loss: 0.1313, Validation Accuracy: 94.55%\n",
            "Epoch 50/50, Train Loss: 0.1252, Validation Loss: 0.1101, Validation Accuracy: 94.55%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=50, lr=0.0001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VdYM3Oc8jKa",
        "outputId": "5c82d92e-f18e-40b5-c854-3f0d64f7cd7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fc1 layer initialized successfully!\n",
            "Epoch 1/50, Train Loss: 2.3968, Validation Loss: 2.3982, Validation Accuracy: 9.09%\n",
            "Epoch 2/50, Train Loss: 2.3966, Validation Loss: 2.3981, Validation Accuracy: 9.09%\n",
            "Epoch 3/50, Train Loss: 2.3964, Validation Loss: 2.3980, Validation Accuracy: 9.09%\n",
            "Epoch 4/50, Train Loss: 2.3961, Validation Loss: 2.3979, Validation Accuracy: 9.09%\n",
            "Epoch 5/50, Train Loss: 2.3959, Validation Loss: 2.3978, Validation Accuracy: 9.09%\n",
            "Epoch 6/50, Train Loss: 2.3957, Validation Loss: 2.3977, Validation Accuracy: 9.09%\n",
            "Epoch 7/50, Train Loss: 2.3955, Validation Loss: 2.3975, Validation Accuracy: 9.09%\n",
            "Epoch 8/50, Train Loss: 2.3952, Validation Loss: 2.3974, Validation Accuracy: 9.09%\n",
            "Epoch 9/50, Train Loss: 2.3950, Validation Loss: 2.3973, Validation Accuracy: 9.09%\n",
            "Epoch 10/50, Train Loss: 2.3947, Validation Loss: 2.3971, Validation Accuracy: 9.09%\n",
            "Epoch 11/50, Train Loss: 2.3945, Validation Loss: 2.3970, Validation Accuracy: 9.09%\n",
            "Epoch 12/50, Train Loss: 2.3942, Validation Loss: 2.3968, Validation Accuracy: 9.09%\n",
            "Epoch 13/50, Train Loss: 2.3939, Validation Loss: 2.3966, Validation Accuracy: 14.55%\n",
            "Epoch 14/50, Train Loss: 2.3936, Validation Loss: 2.3964, Validation Accuracy: 18.18%\n",
            "Epoch 15/50, Train Loss: 2.3932, Validation Loss: 2.3962, Validation Accuracy: 18.18%\n",
            "Epoch 16/50, Train Loss: 2.3929, Validation Loss: 2.3960, Validation Accuracy: 12.73%\n",
            "Epoch 17/50, Train Loss: 2.3925, Validation Loss: 2.3957, Validation Accuracy: 9.09%\n",
            "Epoch 18/50, Train Loss: 2.3922, Validation Loss: 2.3955, Validation Accuracy: 9.09%\n",
            "Epoch 19/50, Train Loss: 2.3917, Validation Loss: 2.3952, Validation Accuracy: 9.09%\n",
            "Epoch 20/50, Train Loss: 2.3912, Validation Loss: 2.3949, Validation Accuracy: 9.09%\n",
            "Epoch 21/50, Train Loss: 2.3908, Validation Loss: 2.3945, Validation Accuracy: 9.09%\n",
            "Epoch 22/50, Train Loss: 2.3903, Validation Loss: 2.3941, Validation Accuracy: 9.09%\n",
            "Epoch 23/50, Train Loss: 2.3897, Validation Loss: 2.3937, Validation Accuracy: 9.09%\n",
            "Epoch 24/50, Train Loss: 2.3892, Validation Loss: 2.3933, Validation Accuracy: 9.09%\n",
            "Epoch 25/50, Train Loss: 2.3886, Validation Loss: 2.3928, Validation Accuracy: 9.09%\n",
            "Epoch 26/50, Train Loss: 2.3880, Validation Loss: 2.3924, Validation Accuracy: 9.09%\n",
            "Epoch 27/50, Train Loss: 2.3872, Validation Loss: 2.3919, Validation Accuracy: 9.09%\n",
            "Epoch 28/50, Train Loss: 2.3867, Validation Loss: 2.3913, Validation Accuracy: 9.09%\n",
            "Epoch 29/50, Train Loss: 2.3859, Validation Loss: 2.3908, Validation Accuracy: 9.09%\n",
            "Epoch 30/50, Train Loss: 2.3852, Validation Loss: 2.3902, Validation Accuracy: 9.09%\n",
            "Epoch 31/50, Train Loss: 2.3845, Validation Loss: 2.3897, Validation Accuracy: 9.09%\n",
            "Epoch 32/50, Train Loss: 2.3836, Validation Loss: 2.3891, Validation Accuracy: 9.09%\n",
            "Epoch 33/50, Train Loss: 2.3829, Validation Loss: 2.3884, Validation Accuracy: 9.09%\n",
            "Epoch 34/50, Train Loss: 2.3819, Validation Loss: 2.3877, Validation Accuracy: 9.09%\n",
            "Epoch 35/50, Train Loss: 2.3809, Validation Loss: 2.3870, Validation Accuracy: 9.09%\n",
            "Epoch 36/50, Train Loss: 2.3800, Validation Loss: 2.3862, Validation Accuracy: 9.09%\n",
            "Epoch 37/50, Train Loss: 2.3791, Validation Loss: 2.3855, Validation Accuracy: 9.09%\n",
            "Epoch 38/50, Train Loss: 2.3779, Validation Loss: 2.3846, Validation Accuracy: 9.09%\n",
            "Epoch 39/50, Train Loss: 2.3769, Validation Loss: 2.3837, Validation Accuracy: 9.09%\n",
            "Epoch 40/50, Train Loss: 2.3759, Validation Loss: 2.3829, Validation Accuracy: 9.09%\n",
            "Epoch 41/50, Train Loss: 2.3746, Validation Loss: 2.3819, Validation Accuracy: 9.09%\n",
            "Epoch 42/50, Train Loss: 2.3737, Validation Loss: 2.3810, Validation Accuracy: 9.09%\n",
            "Epoch 43/50, Train Loss: 2.3722, Validation Loss: 2.3801, Validation Accuracy: 9.09%\n",
            "Epoch 44/50, Train Loss: 2.3710, Validation Loss: 2.3789, Validation Accuracy: 9.09%\n",
            "Epoch 45/50, Train Loss: 2.3691, Validation Loss: 2.3779, Validation Accuracy: 9.09%\n",
            "Epoch 46/50, Train Loss: 2.3679, Validation Loss: 2.3768, Validation Accuracy: 9.09%\n",
            "Epoch 47/50, Train Loss: 2.3665, Validation Loss: 2.3756, Validation Accuracy: 9.09%\n",
            "Epoch 48/50, Train Loss: 2.3647, Validation Loss: 2.3742, Validation Accuracy: 9.09%\n",
            "Epoch 49/50, Train Loss: 2.3628, Validation Loss: 2.3732, Validation Accuracy: 9.09%\n",
            "Epoch 50/50, Train Loss: 2.3612, Validation Loss: 2.3716, Validation Accuracy: 9.09%\n",
            "Model fine-tuning complete and saved!\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = len(os.listdir('/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/train'))\n",
        "\n",
        "# Initialize the model\n",
        "model = FaceRecognitionModel(num_classes=num_classes).to(device)\n",
        "\n",
        "sample_input_shape = (1, 3, 128, 128)  # Batch size 1, RGB image of size 128x128\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "\n",
        "print(\"Model fc1 layer initialized successfully!\")\n",
        "\n",
        "\n",
        "model.fine_tune(train_loader, val_loader, num_epochs=50, lr=0.00001)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'face_recognition_model.pth')\n",
        "print(\"Model fine-tuning complete and saved!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zOLAxDffdAS",
        "outputId": "d00867d3-fce0-4ba3-8c12-f5f6cead3ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed and saved: /content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/Animesh_Face/annotated_1676639882315.png\n",
            "Processed and saved: /content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/Animesh_Face/annotated_1720390672972.jpg\n",
            "Processed and saved: /content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/Animesh_Face/annotated_IMG_20230217_123800.jpg\n",
            "Processed and saved: /content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/Animesh_Face/annotated_IMG_20221024_091759.jpg\n",
            "Processed and saved: /content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/Animesh_Face/annotated_1723174231715.jpg\n",
            "All images processed and annotated!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Mount Google Drive (if using it)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the image folder and output folder (adjust paths)\n",
        "image_folder = \"/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/data/val/Animesh\"  # Update with your folder path\n",
        "output_folder = \"/content/drive/Shareddrives/CNN_Project/Attendance_Management/Photos2026/Animesh_Face\"  # Update with your folder path\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Define class labels\n",
        "class_labels = [\"Animesh\", \"Ankita\", \"Anmol\", \"Deependra\", \"Narayan\", \"Pankaj\", \"Sahil\", \"Shivam\", \"Sonal\", \"Suraj\", \"Vidya\"]\n",
        "# Load the fine-tuned model\n",
        "model = FaceRecognitionModel(num_classes=len(class_labels))  # Update number of classes\n",
        "# # Dynamically initialize fc1 with the same input shape used during training\n",
        "sample_input_shape = (1, 3, 128, 128)  # Replace with your input dimensions\n",
        "model.initialize_fc1(sample_input_shape)\n",
        "model.load_state_dict(torch.load(\"/content/face_recognition_model.pth\", map_location=\"cpu\"))  # Adjust path\n",
        "model.eval()\n",
        "\n",
        "# Image preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to match model input\n",
        "    transforms.ToTensor(),          # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "\n",
        "# Function to annotate an image\n",
        "def annotate_image(image, label):\n",
        "    annotated_image = image.copy()\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(annotated_image, label, (10, 30), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    return annotated_image\n",
        "\n",
        "# Process each image in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    file_path = os.path.join(image_folder, filename)\n",
        "\n",
        "    # Skip non-image files\n",
        "    if not filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "        continue\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(file_path).convert(\"RGB\")\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predicted_label = class_labels[predicted.item()]\n",
        "\n",
        "    # Convert to OpenCV format for annotation\n",
        "    cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "    annotated_image = annotate_image(cv_image, predicted_label)\n",
        "\n",
        "    # Save the annotated image to the output folder\n",
        "    output_path = os.path.join(output_folder, f\"annotated_{filename}\")\n",
        "    cv2.imwrite(output_path, annotated_image)\n",
        "\n",
        "    print(f\"Processed and saved: {output_path}\")\n",
        "\n",
        "print(\"All images processed and annotated!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}